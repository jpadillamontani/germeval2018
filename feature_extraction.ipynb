{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pickle, gensim, numpy as np\n",
    "\n",
    "from utilities import get_train_data, get_test_data, Tokenizer, find_subtoken\n",
    "\n",
    "PICKLE_FOLDER_PATH = ...\n",
    "\n",
    "TRAIN_FILENAME = ...\n",
    "TEST_FILENAME  = ...\n",
    "\n",
    "#------------------------------\n",
    "#source:\n",
    "#http://www.cl.uni-heidelberg.de/english/research/downloads/resource_pages/GermanTwitterEmbeddings/GermanTwitterEmbeddings_data.shtml\n",
    "MODEL_FILENAME  = \"twitter-de_d100_w5_min10.bin\" # 821,8 MB\n",
    "MODEL_DIMENSION = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "word2vec_model  = gensim.models.KeyedVectors.load_word2vec_format(MODEL_FILENAME, binary=True)\n",
    "\n",
    "X_train, y_train_t1, y_train_t2 = get_train_data(TRAIN_FILENAME)\n",
    "X_test                          = get_test_data(TEST_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NGRAM FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), max_df=0.01, min_df=0.0002,\n",
    "                             preprocessor=Tokenizer(preserve_case=False, join=True).tokenize)\n",
    "\n",
    "token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 3), max_df=0.01, min_df=0.0002,\n",
    "                             tokenizer=Tokenizer(preserve_case=False, use_stemmer=True).tokenize)\n",
    "\n",
    "X_CNGR_train = char_vect.fit_transform(X_train)\n",
    "X_CNGR_test  = char_vect.transform(X_test)\n",
    "\n",
    "X_TNGR_train = token_vect.fit_transform(X_train)\n",
    "X_TNGR_test  = token_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_CNGR_train, open(PICKLE_FOLDER_PATH + \"X_CNGR_train.p\", \"wb\" ))\n",
    "pickle.dump(X_CNGR_test,  open(PICKLE_FOLDER_PATH + \"X_CNGR_test.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_TNGR_train, open(PICKLE_FOLDER_PATH + \"X_TNGR_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TNGR_test,  open(PICKLE_FOLDER_PATH + \"X_TNGR_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMB FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EMB_feats(tweets):   \n",
    "    tknzr = Tokenizer(preserve_case=True)\n",
    "    tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    X_EMB = []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        emb = np.zeros(MODEL_DIMENSION)\n",
    "        extra_tokens = 0\n",
    "        \n",
    "        for token in tweet:\n",
    "            try:\n",
    "                emb += word2vec_model[token]\n",
    "            except:\n",
    "                prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "                    \n",
    "                if prefix != None and suffix != None:\n",
    "                    emb += word2vec_model[prefix] + word2vec_model[suffix]\n",
    "                    extra_tokens += 1\n",
    "                elif prefix != None and suffix == None:\n",
    "                    emb += word2vec_model[prefix]\n",
    "                elif prefix == None and suffix != None:\n",
    "                    emb += word2vec_model[suffix]\n",
    "                    \n",
    "        emb /= (len(tweet) + extra_tokens)\n",
    "        X_EMB.append(emb)\n",
    "        \n",
    "    return normalize(X_EMB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_EMB_train = get_EMB_feats(X_train)\n",
    "X_EMB_test  = get_EMB_feats(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_EMB_train, open(PICKLE_FOLDER_PATH + \"X_EMB_train.p\", \"wb\" ))\n",
    "pickle.dump(X_EMB_test,  open(PICKLE_FOLDER_PATH + \"X_EMB_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TIMP FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_imp_tokenlvl(k, category, max_df=0.01, min_df=0.0002):      \n",
    "    token_vect = TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False,\n",
    "                                 max_df=max_df, min_df=min_df,\n",
    "                                 tokenizer=Tokenizer(preserve_case=True).tokenize)\n",
    "    \n",
    "    tfidf = token_vect.fit_transform(X_train)\n",
    "    \n",
    "    vocab = token_vect.vocabulary_\n",
    "    inv_vocab = {index: word for word, index in vocab.items()}\n",
    "    \n",
    "    if category in ['OTHER', 'OFFENSE']:\n",
    "        cat_ids = np.where(y_train_t1 == category)\n",
    "    elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "        cat_ids = np.where(y_train_t2 == category)\n",
    "        \n",
    "    most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "        \n",
    "    most_imp = []\n",
    "    for index in most_imp_ids:\n",
    "        most_imp.append(inv_vocab[index])\n",
    "\n",
    "    return most_imp[:k]\n",
    "\n",
    "def get_TIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "    feats_max = []\n",
    "    feats_min = []\n",
    "           \n",
    "    imp_tokens_vectors = []\n",
    "    for imp_token in k_most_imp_tokenlvl(k, category, max_df=max_df, min_df=min_df):\n",
    "        try:\n",
    "            imp_tokens_vectors.append(word2vec_model[imp_token])\n",
    "        except:\n",
    "            imp_tokens_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "    \n",
    "    tknzr = Tokenizer(preserve_case=True)\n",
    "    tweets = [tknzr.tokenize(tweet) for tweet in tweets]\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        tweet_vectors = []\n",
    "        for token in tweet:\n",
    "            try:\n",
    "                tweet_vectors.append(word2vec_model[token])\n",
    "            except:\n",
    "                prefix = find_subtoken(token, word2vec_model, mode='initial')\n",
    "                suffix = find_subtoken(token, word2vec_model, mode='final')\n",
    "                 \n",
    "                if prefix != None and suffix != None:\n",
    "                    tweet_vectors.append(word2vec_model[prefix])\n",
    "                    tweet_vectors.append(word2vec_model[suffix])\n",
    "                elif prefix != None and suffix == None:\n",
    "                    tweet_vectors.append(word2vec_model[prefix])\n",
    "                elif prefix == None and suffix != None:\n",
    "                    tweet_vectors.append(word2vec_model[suffix])\n",
    "                else:\n",
    "                    tweet_vectors.append(np.zeros(MODEL_DIMENSION))\n",
    "                    \n",
    "        similarity = cosine_similarity(np.asarray(tweet_vectors), np.asarray(imp_tokens_vectors))\n",
    "        \n",
    "        feats_max.append(np.amax(similarity, axis=0))\n",
    "        feats_min.append(np.amin(similarity, axis=0))\n",
    "        \n",
    "    return np.concatenate((feats_max, feats_min), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N_TIMP_TASK1 = 1250\n",
    "N_TIMP_TASK2 = 170\n",
    "\n",
    "X_TIMP_task1_train = \\\n",
    "np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_TIMP_task1_test = \\\n",
    "np.concatenate((get_TIMP_feats(X_test,  N_TIMP_TASK1, 'OTHER'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_TIMP_task2_train = \\\n",
    "np.concatenate((get_TIMP_feats(X_train, N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_train, N_TIMP_TASK2, 'PROFANITY')), axis=1)\n",
    "\n",
    "X_TIMP_task2_test = \\\n",
    "np.concatenate((get_TIMP_feats(X_test,  N_TIMP_TASK2, 'OTHER'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK2, 'ABUSE'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK2, 'INSULT'),\n",
    "                get_TIMP_feats(X_test,  N_TIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_TIMP_task1_train, open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task1_test,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task1_test.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_train, open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_train.p\", \"wb\" ))\n",
    "pickle.dump(X_TIMP_task2_test,  open(PICKLE_FOLDER_PATH + \"X_TIMP_task2_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIMP FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_most_imp_charlvl(k, category, max_df=0.01, min_df=0.0002):    \n",
    "    char_vect  = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 7), lowercase=False,\n",
    "                                 max_df=max_df, min_df=min_df,\n",
    "                                 preprocessor=Tokenizer(preserve_case=True, join=True).tokenize)\n",
    "\n",
    "    tfidf = char_vect.fit_transform(X_train)\n",
    "    \n",
    "    vocab = char_vect.vocabulary_\n",
    "    inv_vocab = {index: word for word, index in vocab.items()}\n",
    "    \n",
    "    if category in ['OTHER', 'OFFENSE']:\n",
    "        cat_ids = np.where(y_train_t1 == category)\n",
    "    elif category in ['PROFANITY', 'ABUSE', 'INSULT']:\n",
    "        cat_ids = np.where(y_train_t2 == category)       \n",
    "        \n",
    "    most_imp_ids = np.argsort(np.asarray(np.mean(tfidf[cat_ids], axis=0)).flatten())[::-1]\n",
    "        \n",
    "    most_imp = []\n",
    "    for index in most_imp_ids:\n",
    "        most_imp.append(inv_vocab[index])\n",
    "\n",
    "    return most_imp[:k]\n",
    "\n",
    "def get_CIMP_feats(tweets, k, category, max_df=0.01, min_df=0.0002):\n",
    "    feats = np.zeros((len(tweets), k))\n",
    "    for imp_ngram_index, imp_ngram in enumerate(k_most_imp_charlvl(k, category, max_df=max_df, min_df=min_df)):\n",
    "        for tweet_index, tweet in enumerate(tweets):\n",
    "            if tweet.find(imp_ngram) != -1:\n",
    "                feats[tweet_index][imp_ngram_index] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N_CIMP_TASK1 = 3200\n",
    "N_CIMP_TASK2 = 370\n",
    "            \n",
    "X_CIMP_task1_train = \\\n",
    "np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_CIMP_task1_test = \\\n",
    "np.concatenate((get_CIMP_feats(X_test,  N_CIMP_TASK1, 'OTHER'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK1, 'OFFENSE')), axis=1)\n",
    "\n",
    "X_CIMP_task2_train = \\\n",
    "np.concatenate((get_CIMP_feats(X_train, N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_train, N_CIMP_TASK2, 'PROFANITY')), axis=1)\n",
    "\n",
    "X_CIMP_task2_test = \\\n",
    "np.concatenate((get_CIMP_feats(X_test,  N_CIMP_TASK2, 'OTHER'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK2, 'ABUSE'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK2, 'INSULT'),\n",
    "                get_CIMP_feats(X_test,  N_CIMP_TASK2, 'PROFANITY')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_CIMP_task1_train, open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_train.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task1_test,  open(PICKLE_FOLDER_PATH + \"X_CIMP_task1_test.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task2_train, open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_train.p\", \"wb\" ))\n",
    "pickle.dump(X_CIMP_task2_test,  open(PICKLE_FOLDER_PATH + \"X_CIMP_task2_test.p\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
